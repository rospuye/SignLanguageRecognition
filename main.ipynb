{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1: American Sign Language Understanding\n",
    "A project for the course of Autonomous Learning at the University of Aveiro, 2022/2023. This project was developed by Diogo Monteiro (NMec 97606) and Isabel Ros√°rio (NMec 93343). This approach attemps to build a neural network that understands American sign language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in dataset: 27455\n",
      "Shape of each image: (28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'First image of the dataset')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('data/sign_mnist_train.csv')\n",
    "\n",
    "# get number of images in the dataset (should be 27,455)\n",
    "num_images = data.shape[0]\n",
    "print('Number of images in dataset:', num_images)\n",
    "\n",
    "# single out and analyse first image\n",
    "first_image = data.iloc[0, 1:].values # get first image of the dataset\n",
    "first_image = first_image.reshape(28, 28).astype('uint8') # reshape it to a 28x28 matrix\n",
    "print('Shape of each image:', first_image.shape)\n",
    "\n",
    "# show first image\n",
    "plt.imshow(first_image, cmap='gray')\n",
    "plt.title('First image of the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pixel information (0-255 to 0-1)\n",
    "data.iloc[:, 1:] = data.iloc[:, 1:] / 255\n",
    "X, y = data.iloc[:, 1:].values, data.iloc[:, 0].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "\n",
    "    def __init__(self, numInputs, numNeurons):\n",
    "        # random initialization of weights\n",
    "        self.weights = 0.01 * np.random.randn(numInputs, numNeurons) # 0.01 to avoid exploding gradients\n",
    "        self.biases = np.zeros((1, numNeurons)) # biases are initialized to 0\n",
    "    \n",
    "    def forwardPass(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases # Z = XW + B\n",
    "\n",
    "    def backPropagation(self, lossDerivatives): # lossPartialDerivates = partial derivatives of loss function with respect to output of layer\n",
    "        # compute gradients (partial derivatives)\n",
    "        self.weightDerivatives = np.dot(self.inputs.T, lossDerivatives) # dL/dW = dL/dY * dY/dW\n",
    "        self.biasDerivatives = np.sum(lossDerivatives, axis=0, keepdims=True) # dL/dB = dL/dY * dY/dB\n",
    "        self.inputDerivatives = np.dot(lossDerivatives, self.weights.T) # dL/dX = dL/dY * dY/dX\n",
    "\n",
    "class Activation:\n",
    "    # activation functions: they are applied to the output of a layer in order to introduce non-linearity\n",
    "\n",
    "    def forwardPassReLU(self, inputs): # rectified linear unit\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backPropagationReLU(self, lossDerivatives): # derivative of ReLU\n",
    "        self.inputDerivatives = lossDerivatives.copy()\n",
    "        self.inputDerivatives[self.inputs <= 0] = 0\n",
    "\n",
    "    def forwardPassSoftmax(self, inputs): # softmax activation: softmax(x) = exp(x) / sum(exp(x))\n",
    "        exponentials = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exponentials / np.sum(exponentials, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backPropagationSoftmax(self, lossDerivatives): # derivative of softmax\n",
    "        self.inputDerivatives = np.empty_like(lossDerivatives)\n",
    "        for index, (singleOutput, singleLossDerivatives) in enumerate(zip(self.output, lossDerivatives)): # iterate over samples\n",
    "            singleOutput = singleOutput.reshape(-1, 1) # reshape to column vector\n",
    "            jacobian = np.diagflat(singleOutput) - np.dot(singleOutput, singleOutput.T) # compute jacobian matrix\n",
    "            self.inputDerivatives[index] = np.dot(jacobian, singleLossDerivatives) # compute partial derivatives\n",
    "\n",
    "class Loss:\n",
    "    # categorical cross-entropy: it is used to evaluate the performance of a classification model by comparing the predicted output to the true output\n",
    "    # it is a kind of loss function: it measures how well the model is performing\n",
    "\n",
    "    def calculate(self, output, y): # compute loss\n",
    "        sampleLosses = self.forwardPass(output, y) # compute loss values for each sample\n",
    "        dataLoss = np.mean(sampleLosses) # compute mean loss\n",
    "        return dataLoss\n",
    "\n",
    "    def forwardPass(self, yPred, yTrue):\n",
    "        samples = len(yPred) # number of samples in a batch\n",
    "        yPredClipped = np.clip(yPred, 1e-7, 1 - 1e-7) # clip data to prevent division by 0\n",
    "        if len(yTrue.shape) == 1: # if labels are one-hot encoded\n",
    "            correctConfidences = yPredClipped[range(samples), yTrue] # get the confidence of the correct label\n",
    "        elif len(yTrue.shape) == 2: # if labels are integers\n",
    "            correctConfidences = np.sum(yPredClipped * yTrue, axis=1) # get the confidence of the correct label\n",
    "        negativeLogLikelihoods = -np.log(correctConfidences) # compute negative log likelihood\n",
    "        return negativeLogLikelihoods # return loss\n",
    "    \n",
    "    def backPropagation(self, lossDerivatives, yTrue):\n",
    "        samples = len(lossDerivatives) # number of samples\n",
    "        labels = len(lossDerivatives[0]) # number of labels\n",
    "        if len(yTrue.shape) == 1: # if labels are one-hot encoded\n",
    "            yTrue = np.eye(labels)[yTrue] # convert to integer labels\n",
    "        self.inputDerivatives = -yTrue / lossDerivatives # compute gradient\n",
    "        self.inputDerivatives = self.inputDerivatives / samples # normalize gradient\n",
    "\n",
    "class LossActivation:\n",
    "    # combined softmax activation and cross-entropy loss for faster backward step\n",
    "    # this is used for the last layer of the model only\n",
    "    # this makes the backward step faster because it is computed in a single step\n",
    "\n",
    "    def __init__(self):\n",
    "        self.activation = Activation()\n",
    "        self.loss = Loss()\n",
    "\n",
    "    def forwardPass(self, inputs, y_true):\n",
    "        self.activation.forwardPassSoftmax(inputs) # apply softmax activation\n",
    "        self.output = self.activation.output # store output of activation function\n",
    "        return self.loss.calculate(self.output, y_true) # return loss value\n",
    "\n",
    "    def backPropagation(self, lossDerivatives, yTrue):\n",
    "        samples = len(lossDerivatives) # number of samples\n",
    "        if len(yTrue.shape) == 2:\n",
    "            yTrue = np.argmax(yTrue, axis=1) # convert one-hot encoded labels to integers\n",
    "            # one-hot enconded labels mean are labels represented as a vector with a 1 in the position of the label and 0s in the other positions\n",
    "        self.inputDerivatives = lossDerivatives.copy() # copy gradient values\n",
    "        self.inputDerivatives[range(samples), yTrue] -= 1 # subtract 1 from the gradient of the correct label\n",
    "        self.inputDerivatives = self.inputDerivatives / samples # divide by the number of samples\n",
    "\n",
    "class Optimizer:\n",
    "    # stochastic gradient descent: it is used to update the weights of the model in order to minimize the loss function\n",
    "    # it is an optimization algorithm that is used to find the optimal set of weights for the model\n",
    "    # stochastic gradient descent works by updating the weights incrementally for each training sample\n",
    "    \n",
    "    def __init__(self, learningRate=1., decay=0., momentum=0.):\n",
    "        self.learningRate = learningRate # learning rate: it is a hyperparameter that controls how much to update the weights at the end of each batch\n",
    "        self.currentLearningRate = learningRate # current learning rate: it is used to update the learning rate over time\n",
    "        self.decay = decay # decay: it is a hyperparameter that controls how much the learning rate decreases over time\n",
    "        self.iter = 0 # iterations: it is used to keep track of the number of iterations performed\n",
    "        self.momentum = momentum # momentum: it is a hyperparameter that controls how much to take the previous updates into account when performing the current update\n",
    "    \n",
    "    def beforeParameterUpdate(self):\n",
    "        if self.decay: # if decay is bigger than zero\n",
    "            self.currentLearningRate = self.learningRate * (1. / (1. + self.decay * self.iter)) # update the learning rate\n",
    "    \n",
    "    def updateParameters(self, layer):\n",
    "        if self.momentum: # if momentum is bigger than zero\n",
    "            if not hasattr(layer, 'weightMomentums'): # if the layer does not have weightMomentums attribute\n",
    "                layer.weightMomentums = np.zeros_like(layer.weights) # initialize weightMomentums with zeros\n",
    "                layer.biasMomentums = np.zeros_like(layer.biases) # initialize biasMomentums with zeros\n",
    "            weightUpdates = self.momentum * layer.weightMomentums - self.currentLearningRate * layer.weightDerivatives # compute weight updates\n",
    "            layer.weightMomentums = weightUpdates # store weight updates\n",
    "            biasUpdates = self.momentum * layer.biasMomentums - self.currentLearningRate * layer.biasDerivatives # compute bias updates\n",
    "            layer.biasMomentums = biasUpdates # store bias updates\n",
    "        else:\n",
    "            weightUpdates = -self.currentLearningRate * layer.weightDerivatives # compute weight updates\n",
    "            biasUpdates = -self.currentLearningRate * layer.biasDerivatives # compute bias updates\n",
    "        layer.weights += weightUpdates # update weights\n",
    "        layer.biases += biasUpdates # update biases\n",
    "\n",
    "    def afterParameterUpdate(self):\n",
    "        self.iter += 1 # increment iterations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.049, loss: 3.258, lr: 0.05\n",
      "epoch: 100, acc: 0.140, loss: 3.126, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.205, loss: 2.935, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.310, loss: 2.597, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.370, loss: 2.273, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.445, loss: 2.019, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.500, loss: 1.824, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.541, loss: 1.666, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.573, loss: 1.535, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.560, loss: 1.485, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.605, loss: 1.365, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.627, loss: 1.297, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.650, loss: 1.209, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.667, loss: 1.141, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.683, loss: 1.085, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.685, loss: 1.068, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.690, loss: 1.062, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.707, loss: 0.976, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.698, loss: 0.990, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.712, loss: 0.931, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.754, loss: 0.857, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.763, loss: 0.817, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.780, loss: 0.769, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.792, loss: 0.749, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.797, loss: 0.722, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.800, loss: 0.703, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.789, loss: 0.712, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.836, loss: 0.633, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.836, loss: 0.612, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.822, loss: 0.621, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.837, loss: 0.597, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.820, loss: 0.623, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.836, loss: 0.569, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.825, loss: 0.585, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.857, loss: 0.536, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.869, loss: 0.499, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.849, loss: 0.518, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.879, loss: 0.485, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.879, loss: 0.467, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.861, loss: 0.475, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.889, loss: 0.440, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.881, loss: 0.437, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.886, loss: 0.433, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.866, loss: 0.453, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.886, loss: 0.404, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.922, loss: 0.353, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.928, loss: 0.345, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.909, loss: 0.353, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.916, loss: 0.350, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.938, loss: 0.309, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.920, loss: 0.324, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.941, loss: 0.290, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.933, loss: 0.300, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.941, loss: 0.279, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.945, loss: 0.267, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.954, loss: 0.257, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.950, loss: 0.251, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.937, loss: 0.269, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.908, loss: 0.334, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.947, loss: 0.243, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.969, loss: 0.212, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.958, loss: 0.223, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.963, loss: 0.218, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.975, loss: 0.194, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.972, loss: 0.189, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.496, loss: 3.154, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.971, loss: 0.193, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.976, loss: 0.182, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.979, loss: 0.175, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.982, loss: 0.168, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.983, loss: 0.164, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.983, loss: 0.160, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.984, loss: 0.156, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.984, loss: 0.154, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.986, loss: 0.149, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.983, loss: 0.150, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.987, loss: 0.141, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.992, loss: 0.132, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.991, loss: 0.131, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.985, loss: 0.139, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.992, loss: 0.125, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.987, loss: 0.126, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.993, loss: 0.117, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.994, loss: 0.114, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.995, loss: 0.111, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.995, loss: 0.108, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.996, loss: 0.105, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.996, loss: 0.102, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.996, loss: 0.099, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.997, loss: 0.097, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.998, loss: 0.094, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.998, loss: 0.092, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.998, loss: 0.090, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.999, loss: 0.088, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.999, loss: 0.086, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.999, loss: 0.084, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.999, loss: 0.082, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.999, loss: 0.080, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.999, loss: 0.078, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.999, loss: 0.076, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.999, loss: 0.075, lr: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "num_features = X.shape[1]\n",
    "size_layer1 = 256\n",
    "size_layer2 = 26\n",
    "\n",
    "activation = Activation()\n",
    "layer1 = Layer(num_features, size_layer1)\n",
    "layer2 = Layer(size_layer1, size_layer2)\n",
    "loss_activation = LossActivation()\n",
    "optimizer = Optimizer(learningRate=0.05, decay=5e-7)\n",
    "\n",
    "for epoch in range(10001):\n",
    "    layer1.forwardPass(X)\n",
    "    activation.forwardPassReLU(layer1.output)\n",
    "\n",
    "    layer2.forwardPass(layer1.output)\n",
    "    loss = loss_activation.forwardPass(layer2.output, y)\n",
    "\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.currentLearningRate}')\n",
    "    \n",
    "    # backward pass\n",
    "    loss_activation.backPropagation(loss_activation.output, y)\n",
    "    layer2.backPropagation(loss_activation.inputDerivatives)\n",
    "    activation.backPropagationReLU(layer2.inputDerivatives)\n",
    "    layer1.backPropagation(layer2.inputDerivatives)\n",
    "\n",
    "    # update weights and biases\n",
    "    optimizer.beforeParameterUpdate()\n",
    "    optimizer.updateParameters(layer1)\n",
    "    optimizer.updateParameters(layer2)\n",
    "    optimizer.afterParameterUpdate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save resulting weights and biases to file\n",
    "np.savez('model.npz', w1=layer1.weights, b1=layer1.biases, w2=layer2.weights, b2=layer2.biases)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.701\n"
     ]
    }
   ],
   "source": [
    "# read model from file\n",
    "model = np.load('model.npz')\n",
    "w1, b1, w2, b2 = model['w1'], model['b1'], model['w2'], model['b2']\n",
    "\n",
    "# load test data\n",
    "test = pd.read_csv('data/sign_mnist_test.csv')\n",
    "test.iloc[:, 1:] = test.iloc[:, 1:] / 255\n",
    "X_test, y_test = test.iloc[:, 1:].values, test.iloc[:, 0].values\n",
    "\n",
    "# make predictions for test data\n",
    "layer1.forwardPass(X_test)\n",
    "activation.forwardPassReLU(layer1.output)\n",
    "layer2.forwardPass(layer1.output)\n",
    "predictions = np.argmax(layer2.output, axis=1)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'accuracy: {accuracy:.3f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a5a87ee616be0254e3f1af9223138e3faeac65b2c9d91bc22a9fc5a4a8bd8eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
